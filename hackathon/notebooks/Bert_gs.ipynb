{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Bert_gs.ipynb","provenance":[{"file_id":"1_YjUyvmTG6Oi59-qQUWWjPAJyYs6Sb_6","timestamp":1615818947054}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"7qo8kqUFp-cg"},"source":["Inspiration: https://www.tensorflow.org/tutorials/text/classify_text_with_bert"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sCoV41ytp_jr","executionInfo":{"status":"ok","timestamp":1615641730951,"user_tz":-60,"elapsed":26264,"user":{"displayName":"Hugo EHLINGER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXlGxMM9AD9Kya0N22jMdPHr16InUahs41RlKyIQ=s64","userId":"07929703146102772963"}},"outputId":"f07e8251-09f2-4b92-e01b-81dc545e49d0"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z-qVoH3iqD6r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615641747215,"user_tz":-60,"elapsed":42523,"user":{"displayName":"Hugo EHLINGER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXlGxMM9AD9Kya0N22jMdPHr16InUahs41RlKyIQ=s64","userId":"07929703146102772963"}},"outputId":"357eb532-94b6-4ea3-fc26-7cf57e0c2b19"},"source":["!pip install -q tensorflow-text\n","!pip install -q tf-models-official"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 3.4MB 19.4MB/s \n","\u001b[K     |████████████████████████████████| 1.1MB 16.4MB/s \n","\u001b[K     |████████████████████████████████| 51kB 9.3MB/s \n","\u001b[K     |████████████████████████████████| 645kB 55.1MB/s \n","\u001b[K     |████████████████████████████████| 37.6MB 78kB/s \n","\u001b[K     |████████████████████████████████| 1.2MB 30.6MB/s \n","\u001b[K     |████████████████████████████████| 174kB 56.6MB/s \n","\u001b[K     |████████████████████████████████| 706kB 48.5MB/s \n","\u001b[K     |████████████████████████████████| 358kB 35.3MB/s \n","\u001b[K     |████████████████████████████████| 102kB 15.6MB/s \n","\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"v_h5eHXfqLeP"},"source":["import os\n","import shutil\n","import itertools as it\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","import tensorflow_addons as tfa\n","from official.nlp import optimization  # to create AdamW optmizer\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import seaborn as sns\n","import numpy as np\n","\n","from sklearn.metrics import confusion_matrix, roc_curve, auc\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","tf.get_logger().setLevel('ERROR')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TYafSrFFqQJC"},"source":["# Data importation"]},{"cell_type":"code","metadata":{"id":"M-egDbsuqOUt"},"source":["os.chdir('/content/drive/MyDrive/Capgemini/Hackathon/notebooks')\n","Xy = pd.read_csv(os.path.join('..', 'data', 'train.csv'), index_col=['id'])\n","X_test = pd.read_csv(os.path.join('..', 'data', 'test.csv'), index_col=['id'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s8dW2O-UrszK"},"source":["# Bert imports"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5b5ywB8Bq1Dv","executionInfo":{"status":"ok","timestamp":1615641750615,"user_tz":-60,"elapsed":45909,"user":{"displayName":"Hugo EHLINGER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXlGxMM9AD9Kya0N22jMdPHr16InUahs41RlKyIQ=s64","userId":"07929703146102772963"}},"outputId":"31b3529e-4f46-48fb-c30e-c8db98eec5ee"},"source":["bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'\n","# bert_model_name = 'bert_en_uncased_L-12_H-768_A-12' \n","\n","map_name_to_handle = {\n","    'bert_en_uncased_L-12_H-768_A-12': 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n","    'bert_en_cased_L-12_H-768_A-12': 'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n","    'bert_multi_cased_L-12_H-768_A-12': 'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n","    'small_bert/bert_en_uncased_L-2_H-128_A-2': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-2_H-256_A-4': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-2_H-512_A-8': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-2_H-768_A-12': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-4_H-128_A-2': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-4_H-256_A-4': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-4_H-512_A-8': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-4_H-768_A-12': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-6_H-128_A-2': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-6_H-256_A-4': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-6_H-512_A-8': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-6_H-768_A-12': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-8_H-128_A-2': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-8_H-256_A-4': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-8_H-512_A-8': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-8_H-768_A-12': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-10_H-128_A-2': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-10_H-256_A-4': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-10_H-512_A-8': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-10_H-768_A-12': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-12_H-128_A-2': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-12_H-256_A-4': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-12_H-512_A-8': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-12_H-768_A-12': 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n","    'albert_en_base': 'https://tfhub.dev/tensorflow/albert_en_base/2',\n","    'electra_small': 'https://tfhub.dev/google/electra_small/2',\n","    'electra_base': 'https://tfhub.dev/google/electra_base/2',\n","    'experts_pubmed': 'https://tfhub.dev/google/experts/bert/pubmed/2',\n","    'experts_wiki_books': 'https://tfhub.dev/google/experts/bert/wiki_books/2',\n","    'talking-heads_base': 'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n","}\n","\n","map_model_to_preprocess = {\n","    'bert_en_uncased_L-12_H-768_A-12': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'bert_en_cased_L-12_H-768_A-12': 'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-128_A-2': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-256_A-4': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-512_A-8': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-768_A-12': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-128_A-2': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-256_A-4': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-512_A-8': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-768_A-12': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-128_A-2': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-256_A-4': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-512_A-8': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-768_A-12': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-128_A-2': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-256_A-4': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-512_A-8': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-768_A-12': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-128_A-2': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-256_A-4': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-512_A-8': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-768_A-12': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-128_A-2': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-256_A-4': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-512_A-8': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-768_A-12': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'bert_multi_cased_L-12_H-768_A-12': 'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n","    'albert_en_base': 'https://tfhub.dev/tensorflow/albert_en_preprocess/2',\n","    'electra_small': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'electra_base': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'experts_pubmed': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'experts_wiki_books': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'talking-heads_base': 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","}\n","\n","tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n","tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n","\n","print(f'BERT model selected           : {tfhub_handle_encoder}')\n","print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_cclN9n3r0bE"},"source":["# Modeling functions"]},{"cell_type":"code","metadata":{"id":"oniKuBLy3_LL"},"source":["# from embedder.src.universal_hp_optimizer import UniversalHPOptimizer\n","import tensorflow as tf\n","import tensorflow.compat.v1 as tfc\n","from tensorboard.plugins.hparams import api as hp\n","import datetime\n","import numpy as np\n","\n","DEFAULT_PARAMS = {\n","    \"epochs\": [10],\n","    'batch_size': [32],\n","    'batch_normalization': [False],\n","    'dropout': [0],\n","    'optimizer': ['adam'],\n","    'early_stopping': [3],\n","    'nb_columns': [-1]\n","}\n","\n","class UniversalHPOptimizer():\n","    \"\"\" A class to optimize a given model and a given dictionary of parameters\n","    to test. It uses the Tensorboard API in order to log the results.\n","    \n","    \"\"\"\n","\n","    def __init__(self, dict_params, create_model, log_dir=\"logs/fit/\", print_summary=False):\n","        \"\"\"\n","        Args:\n","            dict_params ([dict]): [Dictionary which values are lists of possible\n","             values for each parameter to test in the GridSearch.]\n","            create_model ([funct]): [Function to create the model.]\n","            print_summary (bool, optional): [Whether to print a summary of the model.\n","            ]. Defaults to False.\n","        \"\"\"\n","        self.user_input = dict_params\n","        self.create_model = create_model\n","        self.print_summary=print_summary\n","        self.best_model = None\n","        self.best_accuracy = None\n","        self.METRIC_ACCURACY = 'accuracy'\n","        self.METRIC_F1 = 'f1-score'\n","        self.log_dir = log_dir\n","        self.params = self.generate_hp_dict(dict_params)\n","\n","\n","    def generate_hp_dict(self, dict_params):\n","        \n","        ret = {}\n","        for k, v in DEFAULT_PARAMS.items():\n","            if k not in dict_params.keys():\n","                ret[k] = v\n","            else:\n","                ret[k] = dict_params[k]\n","        \n","        for k, v in dict_params.items():\n","            if k not in ret:\n","                ret[k] = v\n","\n","        return ret\n","\n","\n","    def run_all(self, x_train, x_test, y_train=None, y_test=None):\n","        \"\"\"[summary]\n","\n","        Args:\n","          x_train ([ndarray]): [Array containing the training images.]\n","          y_train ([ndarray]): [Array containing the labels of the training \n","          images.]\n","          x_test ([ndarray]): [Array containing the training images.]\n","          y_test ([ndarray]): [Array containing the labels of the training \n","          images.]\n","        \"\"\"\n","\n","        session_num = 1\n","\n","        keys, values = zip(*self.params.items())\n","        combinations = [dict(zip(keys, v)) for v in it.product(*values)]\n","\n","        for hparams in combinations:\n","            run_name = \"run-%d\" % session_num\n","            print('\\n--- Starting trial: %s' % run_name)\n","            print({k: v for k, v in hparams.items()})\n","            self.run(hparams, x_train, y_train, x_test, y_test, 'logs/hparam_tuning/' + run_name)\n","            session_num += 1\n","\n","\n","    def train_test_model(self, hparams, x_train, y_train, x_test, y_test):\n","        \"\"\"[summary]\n","\n","        Args:\n","            hparams ([dict]): [A dictionary of model parameters. The available\n","            parameters are :\n","            - batch_normalization (boolean): Adds two Batch normalization layers\n","            before the dropout layers. \n","            - optimizer (str) : Type of optimizer to use.\n","            - dropout (float) : The dropout rate, between 0 and 1.\n","            - batch_size (int): size of the batch.\n","            - epoch (int): number of epochs.\n","            ]\n","            x_train ([ndarray]): [Array containing the training images.]\n","            y_train ([ndarray]): [Array containing the labels of the training \n","            images.]\n","            x_test ([ndarray]): [Array containing the training images.]\n","            y_test ([ndarray]): [Array containing the labels of the training \n","            images.]\n","\n","        Returns:\n","            [float]: [Returns the scalar test loss of the test.]\n","        \"\"\"\n","        model = self.create_model(hparams, self.print_summary)\n","        log_dir = os.path.join(self.log_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","\n","        if hparams['nb_columns'] != -1:\n","            x_train = x_train.iloc[:, 0:hparams['nb_columns']]\n","            x_test = x_test.iloc[:, 0:hparams['nb_columns']]\n","\n","        params_callback = {}\n","        for k, v in hparams.items():\n","            if k in self.user_input:\n","                params_callback[k] = v\n","\n","        # tensorboard_callback = [tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1),\n","        #                         hp.KerasCallback(log_dir, params_callback)]\n","        #                         tf.keras.callbacks.EarlyStopping(monitor='f1-score', patience=hparams[\"early_stopping\"], verbose=1)]\n","\n","        tensorboard_callback = [tf.keras.callbacks.EarlyStopping(monitor='f1-score', patience=hparams[\"early_stopping\"], verbose=1)]\n","\n","        if y_train is None or y_test is None:\n","            # Assume batch dataset\n","            model.fit(x_train, batch_size=hparams[\"batch_size\"], epochs=hparams[\"epochs\"], validation_data=x_test) \n","            _, accuracy = model.evaluate(x_test)\n","        else:\n","            print(f\"x_train shape = {x_train.shape}, y_test shape = {y_train.shape}\")\n","            model.fit(x_train, y_train, callbacks=tensorboard_callback, batch_size=hparams[\"batch_size\"], epochs=hparams[\"epochs\"], validation_data=(x_test, y_test))\n","            print(f\"x_test shape = {x_test.shape}, y_test shape = {y_test.shape}\")\n","            _, accuracy = model.evaluate(x_test, y_test)\n","        \n","        if isinstance(accuracy, np.ndarray):\n","            accuracy = sum(accuracy) / len(accuracy)\n","\n","        print(f\"Accuracy found = {accuracy}\")\n","        print(f\"Best Accuracy found = {self.best_accuracy}\")\n","        if self.best_accuracy is None or accuracy > self.best_accuracy:\n","            self.best_accuracy = accuracy\n","            self.best_model = model\n","            self.best_params = hparams\n","\n","        return accuracy\n","        # return 0\n","\n","    def run(self, hparams, x_train, y_train, x_test, y_test, run_dir=\".\"):\n","        \"\"\"[summary]\n","\n","        Args:\n","            hparams ([dict]): [A dictionary of model parameters. The available\n","            parameters are :\n","            - batch_normalization (boolean): Adds two Batch normalization layers\n","            before the dropout layers. \n","            - optimizer (str) : Type of optimizer to use.\n","            - dropout (float) : The dropout rate, between 0 and 1.\n","            - batch_size (int): size of the batch.\n","            - epoch (int): number of epochs.\n","            ]\n","            x_train ([ndarray]): [Array containing the training images.]\n","            y_train ([ndarray]): [Array containing the labels of the training \n","            images.]\n","            x_test ([ndarray]): [Array containing the training images.]\n","            y_test ([ndarray]): [Array containing the labels of the training \n","            images.]\n","            run_dir (str, optional): [Path to the directory from which to run \n","            the model.]. Defaults to \".\".\n","        \"\"\"\n","\n","        with tf.summary.create_file_writer(run_dir).as_default():\n","            hp.hparams(hparams)  # record the values used in this trial\n","            accuracy = self.train_test_model(hparams, x_train, y_train, x_test, y_test)\n","            # tf.summary.scalar(self.METRIC_ACCURACY, accuracy, step=1)\n","\n"," \n","    def predict(self, x_test):\n","        \"\"\" Predicts the results for x_test with the model.\n","\n","        Args:\n","            x_test ([ndarray]): [Array containing the training images.]\n","\n","        Returns:\n","            [tuple]: [Returns an array of weights for predictions and an \n","            array of predicted labels.]\n","        \"\"\"\n","        predicted_probas = self.best_model.predict(x_test, verbose=1, max_queue_size=10)\n","        predicted_classes = np.argmax(predicted_probas, axis=-1)\n","        return predicted_probas, predicted_classes\n","\n","\n","    def get_confusion_matrix(self, y_true, x_test=None, y_pred=None, labels=None):\n","        \"\"\" Builds a confusion matrix for the model.\n","\n","        Args:\n","            y_true ([ndarray]): [Array containing the true labels.]\n","            x_test ([ndarray]): [Array containing the training images.]. Defaults to None.\n","            y_pred ([ndarray]): [Array containing the weigths for the prediction.]. Defaults to None.\n","            labels ([list], optional): [List containing all of the unique labels.]. Defaults to None. \n","\n","        Returns:\n","            [DataFrame]: [Returns a pandas DataFrame containing the confusion matrix, \n","            with the rows being the true labels and the columns the predicted labels.]\n","        \"\"\"\n","        if labels is not None:\n","            real_labels = [\"real \" + label for label in labels]\n","            pred_labels = [\"pred \" + label for label in labels]\n","        else:\n","            real_labels, pred_labels = None, None\n","\n","        if y_pred is None:\n","            _, y_pred = self.predict(x_test)\n","        conf_matrix = tf.math.confusion_matrix(y_true, y_pred).numpy()\n","        if real_labels is not None and pred_labels is not None:\n","            conf_matrix = pd.DataFrame(conf_matrix, index=real_labels, columns=pred_labels)\n","        return conf_matrix\n","\n","\n","    def plot_evaluation_info(self, conf_matrix):\n","        \"\"\" Shows 3 confusion matrices for the model.\n","\n","        Args:\n","            conf_matrix ([ndarray or pd.DataFrame]): [confusion matrix with absolute values]\n","\n","        Returns: [None]\n","        \"\"\"\n","\n","        fig, ax = plt.subplots(ncols=3, nrows=1, figsize=(20, 5))\n","        conf_matrix_norm_recall = conf_matrix / (np.sum(conf_matrix, axis=1).to_numpy().reshape(len(conf_matrix), 1))\n","        conf_matrix_norm_precision = conf_matrix / np.sum(conf_matrix, axis=0)\n","        \n","        sns.heatmap(conf_matrix, annot=True, cmap='Blues', ax=ax[0]).set_title(\"Absolute value confusion matrix\")\n","        sns.heatmap(conf_matrix_norm_recall, annot=True, fmt='.2%', cmap='Blues', ax=ax[1]).set_title(\"Recall confusion matrix\")\n","        sns.heatmap(conf_matrix_norm_precision, annot=True, fmt='.2%', cmap='Blues', ax=ax[2]).set_title(\"Precision confusion matrix\")\n","        plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8UMqQ4aMCaMD"},"source":["def load_data_grid(Xy, X_test, column):\n","\n","    Xy_train_column = Xy.dropna(subset=[column], inplace=False)\n","    X_test_column = X_test.dropna(subset=[column], inplace=False)\n","    X_test_column = X_test_column[column]\n","    X_train_column = Xy_train_column[column]\n","    y_train_column = Xy_train_column['class']\n","    \n","    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n","    for train_index, test_index in sss.split(X_train_column, y_train_column):\n","        X_train_column, X_val = X_train_column.iloc[train_index], X_train_column.iloc[test_index]\n","        y_train_column, y_val = y_train_column.iloc[train_index], y_train_column.iloc[test_index]\n","\n","    y_train_cat, y_val_cat = tf.keras.utils.to_categorical(y_train_column), tf.keras.utils.to_categorical(y_val)\n","\n","    return X_train_column, y_train_cat, X_val, y_val_cat, X_test_column\n","\n","\n","def load_data(Xy_train, X_test, column):\n","\n","    Xy_train_column = Xy_train.dropna(subset=[column], inplace=False)\n","    X_test_column = X_test.dropna(subset=[column], inplace=False)\n","    X_test_column = X_test_column[column]\n","    X_train_column = Xy_train_column[column]\n","    y_train_column = Xy_train_column['class']\n","    y_train_column_cat = tf.keras.utils.to_categorical(y_train_column)\n","\n","    return X_train_column, y_train_column_cat, X_test_column"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6K1oQQUP-c4h"},"source":["def train_model(X_train, y_train_cat, tfhub_handle_encoder, tfhub_handle_preprocess, model_name, input_length=44, epochs=15):\n","\n","    def build_classifier_model():\n","        text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n","        preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n","        encoder_inputs = preprocessing_layer(text_input)\n","        encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n","        outputs = encoder(encoder_inputs)\n","        net = outputs['pooled_output']\n","        net = tf.keras.layers.Dropout(0.1)(net)\n","        net = tf.keras.layers.Dense(4, activation=tf.keras.activations.softmax, name='classifier')(net)\n","        return tf.keras.Model(text_input, net)\n","\n","    bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n","    bert_model = hub.KerasLayer(tfhub_handle_encoder)\n","\n","    classifier_model = build_classifier_model()\n","    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n","    metrics = tfa.metrics.F1Score(num_classes=4)\n","\n","    steps_per_epoch = len(X_train)\n","    # num_train_steps = steps_per_epoch * epochs\n","    num_train_steps = int(len(X_train) / 32)\n","    num_warmup_steps = int(0.1 * num_train_steps)\n","\n","    init_lr = 3e-5\n","    optimizer = optimization.create_optimizer(init_lr=init_lr,\n","                                            num_train_steps=num_train_steps,\n","                                            num_warmup_steps=num_warmup_steps,\n","                                            optimizer_type='adamw')\n","    \n","    classifier_model.compile(optimizer=optimizer,\n","                         loss=loss,\n","                         metrics=metrics)\n","\n","    print(f'Training model with {tfhub_handle_encoder}')\n","    history = classifier_model.fit(X_train, y_train_cat, 32, epochs=epochs, validation_split=0.2)\n","\n","    return classifier_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Km5f53bv_VE2"},"source":["def predict_and_save(classifier_model, X_test, filename_submission, filename_unpack, filename_model, labels):\n","\n","    y_pred_cat = classifier_model.predict(\n","        X_test, batch_size=None, verbose=1, steps=None, callbacks=None, max_queue_size=10,\n","        workers=1, use_multiprocessing=False)\n","    \n","    y_pred = np.argmax(y_pred_cat, axis=1)\n","\n","    y_pred_submission = pd.DataFrame(y_pred, index=X_test.index.values, columns=['class'])\n","    y_pred_submission.index.rename('id', inplace=True)\n","    y_pred_submission.to_csv(os.path.join('..', 'data', filename_submission))\n","\n","    y_pred_cat_df = pd.DataFrame(y_pred_cat, index=X_test.index.values, columns=labels)\n","    y_pred_cat_df.index.rename('id', inplace=True)\n","    y_pred_cat_df.to_csv(os.path.join('..', 'data', filename_unpack))\n","\n","    classifier_model.save(os.path.join('..', 'saved_models', filename_model))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rek3dSicAw9B"},"source":["# Execution"]},{"cell_type":"markdown","metadata":{"id":"Ml7hJYTpA781"},"source":["## Title"]},{"cell_type":"code","metadata":{"id":"Siu-LYtT-Opr"},"source":["# X_train_title, y_train_title_cat, X_val_title, y_val_title_cat, X_test_title = load_data(Xy, X_test, 'title')\n","# classifier_model_title = train_model(X_train_title, y_train_title_cat, tfhub_handle_encoder, tfhub_handle_preprocess, input_length=44)\n","# predict_and_save(classifier_model_title, X_test_title, 'title_submission3.csv', 'title_softmax3.csv', 'title_big_bert', labels=['title_0', 'title_1', 'title_2', 'title_3'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hHDNdYccC91e"},"source":["## Description"]},{"cell_type":"code","metadata":{"id":"cI7wBZMICXOj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615646859433,"user_tz":-60,"elapsed":359153,"user":{"displayName":"Hugo EHLINGER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXlGxMM9AD9Kya0N22jMdPHr16InUahs41RlKyIQ=s64","userId":"07929703146102772963"}},"outputId":"2646019a-06e1-4637-9bdf-9fb4d115e495"},"source":["# Loading data\n","X_train_desc, y_train_desc_cat, X_test_desc = load_data(Xy, X_test, 'description')\n","# Preprocessing\n","# test_preprocess_model = make_bert_preprocess_model(['description'], seq_length=172)\n","# text_preprocessed = test_preprocess_model(X_train_desc)\n","# Train model\n","classifier_model_desc = train_model(X_train_desc, y_train_desc_cat, tfhub_handle_encoder, tfhub_handle_preprocess, epochs=10)\n","predict_and_save(classifier_model_desc, X_test_desc, 'desc_submission_numtrainsteps.csv', 'desc_softmax_numtrainsteps.csv', 'desc_big_bert_numtrainsteps', labels=['desc_0', 'desc_1', 'desc_2', 'desc_3'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n","Epoch 1/10\n","135/135 [==============================] - 38s 248ms/step - loss: 1.0765 - f1_score: 0.5634 - val_loss: 0.3107 - val_f1_score: 0.8867\n","Epoch 2/10\n","135/135 [==============================] - 33s 245ms/step - loss: 0.3205 - f1_score: 0.8858 - val_loss: 0.2905 - val_f1_score: 0.8928\n","Epoch 3/10\n","135/135 [==============================] - 33s 246ms/step - loss: 0.3088 - f1_score: 0.8911 - val_loss: 0.2905 - val_f1_score: 0.8928\n","Epoch 4/10\n","135/135 [==============================] - 33s 244ms/step - loss: 0.3130 - f1_score: 0.8899 - val_loss: 0.2905 - val_f1_score: 0.8928\n","Epoch 5/10\n","135/135 [==============================] - 33s 244ms/step - loss: 0.3186 - f1_score: 0.8888 - val_loss: 0.2905 - val_f1_score: 0.8928\n","Epoch 6/10\n","135/135 [==============================] - 33s 243ms/step - loss: 0.3318 - f1_score: 0.8846 - val_loss: 0.2905 - val_f1_score: 0.8928\n","Epoch 7/10\n","135/135 [==============================] - 33s 242ms/step - loss: 0.3216 - f1_score: 0.8879 - val_loss: 0.2905 - val_f1_score: 0.8928\n","Epoch 8/10\n","135/135 [==============================] - 33s 241ms/step - loss: 0.2967 - f1_score: 0.8959 - val_loss: 0.2905 - val_f1_score: 0.8928\n","Epoch 9/10\n","135/135 [==============================] - 33s 243ms/step - loss: 0.3035 - f1_score: 0.9022 - val_loss: 0.2905 - val_f1_score: 0.8928\n","Epoch 10/10\n","135/135 [==============================] - 33s 244ms/step - loss: 0.3073 - f1_score: 0.8922 - val_loss: 0.2905 - val_f1_score: 0.8928\n","42/42 [==============================] - 5s 100ms/step\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 310). These functions will not be directly callable after loading.\n","WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 310). These functions will not be directly callable after loading.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"B83mVbDGr26P"},"source":["# classifier_model_desc.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7qsYDXgr4KEB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8eOe-q8T4MRN"},"source":["## GridSearch - HP Params"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q9Goc4So4Pmc","executionInfo":{"status":"ok","timestamp":1615646003715,"user_tz":-60,"elapsed":711365,"user":{"displayName":"Hugo EHLINGER","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjXlGxMM9AD9Kya0N22jMdPHr16InUahs41RlKyIQ=s64","userId":"07929703146102772963"}},"outputId":"87e78b9d-f3b0-4661-83bd-9ffdec2d5ece"},"source":["def generate_model(hparams, print_summary=False):\n","\n","    def build_classifier_model():\n","        text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n","        preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n","        encoder_inputs = preprocessing_layer(text_input)\n","        encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n","        outputs = encoder(encoder_inputs)\n","        net = outputs['pooled_output']\n","        net = tf.keras.layers.Dropout(hparams['dropout'])(net)\n","        net = tf.keras.layers.Dense(4, activation=tf.keras.activations.softmax, name='classifier')(net)\n","        return tf.keras.Model(text_input, net)\n","\n","    bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n","    bert_model = hub.KerasLayer(tfhub_handle_encoder)\n","\n","    classifier_model = build_classifier_model()\n","    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n","    metrics = tfa.metrics.F1Score(num_classes=4)\n","\n","    # steps_per_epoch = len(X_train)\n","    num_train_steps = hparams['steps_per_epoch'] * hparams['epochs']\n","    num_warmup_steps = int(0.1 * num_train_steps)\n","\n","    init_lr = 3e-5\n","    optimizer = optimization.create_optimizer(init_lr=init_lr,\n","                                            num_train_steps=num_train_steps,\n","                                            num_warmup_steps=num_warmup_steps,\n","                                            optimizer_type='adamw')\n","    \n","    classifier_model.compile(optimizer=optimizer,\n","                         loss=loss,\n","                         metrics=metrics)\n","\n","    return classifier_model\n","\n","logs = os.path.join(\"..\", \"logs\")\n","X_train_title, y_train_title, X_val_title, y_val_title, X_test_title = load_data(Xy, X_test, 'title')\n","X_train_desc, y_train_desc, X_val_desc, y_val_desc, X_test_desc = load_data(Xy, X_test, 'description')\n","\n","params = {\n","    \"epochs\": [10],\n","    'early_stopping': [2],\n","    'dropout': [0.1],\n","    'steps_per_epoch': [len(X_train_title), int(len(X_train_title) / 32)],\n","    'batch_size': [16],\n","}\n","\n","print(f\"Shape X_train = {X_train_title.shape} \\n X_val = {X_val_title.shape} \\n y_train = {y_train_title.shape} \\n y_val= {y_val_title.shape}\")\n","hp_han = UniversalHPOptimizer(params, generate_model, log_dir=logs, print_summary=True)\n","hp_han.run_all(x_train=X_train_desc, x_test=X_val_desc, y_train=y_train_desc, y_test=y_val_desc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Shape X_train = (5393,) \n"," X_val = (1349,) \n"," y_train = (5393, 4) \n"," y_val= (1349, 4)\n","\n","--- Starting trial: run-1\n","{'epochs': 10, 'batch_size': 16, 'batch_normalization': False, 'dropout': 0.1, 'optimizer': 'adam', 'early_stopping': 2, 'nb_columns': -1, 'steps_per_epoch': 5393}\n","x_train shape = (4289,), y_test shape = (4289, 4)\n","Epoch 1/10\n","269/269 [==============================] - 40s 128ms/step - loss: 1.5577 - f1_score: 0.1926 - val_loss: 1.1556 - val_f1_score: 0.4845\n","Epoch 2/10\n","269/269 [==============================] - 34s 125ms/step - loss: 1.0637 - f1_score: 0.5401 - val_loss: 0.5873 - val_f1_score: 0.8172\n","Epoch 3/10\n","269/269 [==============================] - 34s 125ms/step - loss: 0.5900 - f1_score: 0.7981 - val_loss: 0.3295 - val_f1_score: 0.8967\n","Epoch 4/10\n","269/269 [==============================] - 34s 125ms/step - loss: 0.3621 - f1_score: 0.8724 - val_loss: 0.2549 - val_f1_score: 0.9229\n","Epoch 5/10\n","269/269 [==============================] - 33s 124ms/step - loss: 0.2808 - f1_score: 0.9075 - val_loss: 0.2347 - val_f1_score: 0.9199\n","Epoch 6/10\n","269/269 [==============================] - 33s 123ms/step - loss: 0.2515 - f1_score: 0.9084 - val_loss: 0.2670 - val_f1_score: 0.9201\n","Epoch 7/10\n","269/269 [==============================] - 33s 124ms/step - loss: 0.1875 - f1_score: 0.9376 - val_loss: 0.2486 - val_f1_score: 0.9272\n","Epoch 8/10\n","269/269 [==============================] - 33s 124ms/step - loss: 0.1603 - f1_score: 0.9408 - val_loss: 0.2485 - val_f1_score: 0.9189\n","Epoch 9/10\n","269/269 [==============================] - 33s 125ms/step - loss: 0.1459 - f1_score: 0.9457 - val_loss: 0.2734 - val_f1_score: 0.9323\n","Epoch 10/10\n","269/269 [==============================] - 34s 125ms/step - loss: 0.1186 - f1_score: 0.9601 - val_loss: 0.3201 - val_f1_score: 0.9259\n","x_test shape = (1073,), y_test shape = (1073, 4)\n","34/34 [==============================] - 4s 109ms/step - loss: 0.3201 - f1_score: 0.9259\n","Accuracy found = 0.9258756190538406\n","Best Accuracy found = None\n","\n","--- Starting trial: run-2\n","{'epochs': 10, 'batch_size': 16, 'batch_normalization': False, 'dropout': 0.1, 'optimizer': 'adam', 'early_stopping': 2, 'nb_columns': -1, 'steps_per_epoch': 168}\n","x_train shape = (4289,), y_test shape = (4289, 4)\n","Epoch 1/10\n","269/269 [==============================] - 39s 126ms/step - loss: 1.1030 - f1_score: 0.5043 - val_loss: 0.2451 - val_f1_score: 0.9206\n","Epoch 2/10\n","269/269 [==============================] - 33s 122ms/step - loss: 0.2711 - f1_score: 0.9094 - val_loss: 0.2436 - val_f1_score: 0.9135\n","Epoch 3/10\n","269/269 [==============================] - 33s 123ms/step - loss: 0.1920 - f1_score: 0.9335 - val_loss: 0.2245 - val_f1_score: 0.9346\n","Epoch 4/10\n","269/269 [==============================] - 33s 124ms/step - loss: 0.1461 - f1_score: 0.9491 - val_loss: 0.2452 - val_f1_score: 0.9288\n","Epoch 5/10\n","269/269 [==============================] - 33s 125ms/step - loss: 0.1041 - f1_score: 0.9634 - val_loss: 0.2796 - val_f1_score: 0.9289\n","Epoch 6/10\n","269/269 [==============================] - 33s 123ms/step - loss: 0.0866 - f1_score: 0.9694 - val_loss: 0.2808 - val_f1_score: 0.9280\n","Epoch 7/10\n","269/269 [==============================] - 33s 123ms/step - loss: 0.0816 - f1_score: 0.9720 - val_loss: 0.2817 - val_f1_score: 0.9270\n","Epoch 8/10\n","269/269 [==============================] - 33s 124ms/step - loss: 0.0962 - f1_score: 0.9658 - val_loss: 0.2817 - val_f1_score: 0.9270\n","Epoch 9/10\n","269/269 [==============================] - 33s 123ms/step - loss: 0.0902 - f1_score: 0.9644 - val_loss: 0.2817 - val_f1_score: 0.9270\n","Epoch 10/10\n","269/269 [==============================] - 33s 123ms/step - loss: 0.0811 - f1_score: 0.9686 - val_loss: 0.2817 - val_f1_score: 0.9270\n","x_test shape = (1073,), y_test shape = (1073, 4)\n","34/34 [==============================] - 3s 102ms/step - loss: 0.2817 - f1_score: 0.9270\n","Accuracy found = 0.927030012011528\n","Best Accuracy found = 0.9258756190538406\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yK2ArYj6_XfO"},"source":[""],"execution_count":null,"outputs":[]}]}