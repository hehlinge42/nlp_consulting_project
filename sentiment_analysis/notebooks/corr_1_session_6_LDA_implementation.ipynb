{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XHEC - Session 6-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic extraction + sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Implementation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session we will build an LDA from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdocs = ['eat turkey on turkey day holiday',\n",
    "           'i like to eat cake on holiday',\n",
    "           'turkey trot race on thanksgiving holiday',\n",
    "           'snail race the turtle',\n",
    "           'time travel space race',\n",
    "           'movie on thanksgiving',\n",
    "           'movie at air and space museum is cool movie',\n",
    "           'aspiring movie star']\n",
    "\n",
    "rawdocs = list(map(lambda x: x.split(), rawdocs)) #Split by whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['eat', 'turkey', 'on', 'turkey', 'day', 'holiday'],\n",
       " ['i', 'like', 'to', 'eat', 'cake', 'on', 'holiday'],\n",
       " ['turkey', 'trot', 'race', 'on', 'thanksgiving', 'holiday'],\n",
       " ['snail', 'race', 'the', 'turtle'],\n",
       " ['time', 'travel', 'space', 'race'],\n",
       " ['movie', 'on', 'thanksgiving'],\n",
       " ['movie', 'at', 'air', 'and', 'space', 'museum', 'is', 'cool', 'movie'],\n",
       " ['aspiring', 'movie', 'star']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2 #Number of topic\n",
    "alpha = 0.1 #Hyperparameter alpha\n",
    "eta = 0.1 #Hyperparameter eta\n",
    "iterationNb = 3 #Number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to a number problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionnary {id: word}\n",
    "vocab = np.unique(list(itertools.chain.from_iterable(rawdocs)))\n",
    "vocab = {k: v for v, k in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'air': 0,\n",
       " 'and': 1,\n",
       " 'aspiring': 2,\n",
       " 'at': 3,\n",
       " 'cake': 4,\n",
       " 'cool': 5,\n",
       " 'day': 6,\n",
       " 'eat': 7,\n",
       " 'holiday': 8,\n",
       " 'i': 9,\n",
       " 'is': 10,\n",
       " 'like': 11,\n",
       " 'movie': 12,\n",
       " 'museum': 13,\n",
       " 'on': 14,\n",
       " 'race': 15,\n",
       " 'snail': 16,\n",
       " 'space': 17,\n",
       " 'star': 18,\n",
       " 'thanksgiving': 19,\n",
       " 'the': 20,\n",
       " 'time': 21,\n",
       " 'to': 22,\n",
       " 'travel': 23,\n",
       " 'trot': 24,\n",
       " 'turkey': 25,\n",
       " 'turtle': 26}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Swap word for id in each document\n",
    "document = [list(map(lambda x: vocab[x], doc)) for doc in rawdocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 25, 14, 25, 6, 8],\n",
       " [9, 11, 22, 7, 4, 14, 8],\n",
       " [25, 24, 15, 14, 19, 8],\n",
       " [16, 15, 20, 26],\n",
       " [21, 23, 17, 15],\n",
       " [12, 14, 19],\n",
       " [12, 3, 0, 1, 17, 13, 10, 5, 12],\n",
       " [2, 12, 18]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the topic-word matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialiseWordTopicMatrix(vocab, document, K):\n",
    "    #Initialise the word-topic count matrix\n",
    "    TopicWordMatrix = np.zeros((K, len(vocab)))\n",
    "    #Randomly assign topic for each word in each document\n",
    "    topicAssignmentList = [[random.randint(0,K-1) for i in range(len(doc))] for doc in document]\n",
    "\n",
    "    for iDoc, doc in enumerate(document): #For all document\n",
    "        for iToken, wordId in enumerate(doc): #For all token\n",
    "            #Find the topic of the given token\n",
    "            tokenTopic =  topicAssignmentList[iDoc][iToken]\n",
    "            #Update the wordTopicMatrix\n",
    "            TopicWordMatrix[tokenTopic][wordId] += 1\n",
    "    return TopicWordMatrix, topicAssignmentList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicWordMatrix, topicAssignmentList = initialiseWordTopicMatrix(vocab, document, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 3., 3.,\n",
       "        1., 2., 1., 1., 1., 1., 1., 1., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 1., 1., 0., 1., 3., 0., 0., 0., 3., 0., 1., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 2., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TopicWordMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 0, 1, 0, 1],\n",
       " [0, 0, 0, 1, 1, 0, 1],\n",
       " [0, 1, 0, 1, 1, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       " [1, 1, 0]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicAssignmentList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the document-topic matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialiseDocumentTopicMatrix(topicAssignmentList, document):\n",
    "    documentTopicMatrix = np.zeros((len(document), K))\n",
    "    for iDoc in range(len(document)):\n",
    "        for iTopic in range(K):\n",
    "            #Update document matrix topic according to topicAssignmentList\n",
    "            documentTopicMatrix[iDoc][iTopic] = topicAssignmentList[iDoc].count(iTopic)\n",
    "    return documentTopicMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentTopicMatrix = initialiseDocumentTopicMatrix(topicAssignmentList, document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 3.],\n",
       "       [4., 3.],\n",
       "       [2., 4.],\n",
       "       [3., 1.],\n",
       "       [4., 0.],\n",
       "       [3., 0.],\n",
       "       [6., 3.],\n",
       "       [1., 2.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentTopicMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA iterations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ldaModel(K, alpha, eta, iterationNb, document, vocab, TopicWordMatrix, topicAssignmentList, documentTopicMatrix):\n",
    "    #For each iteration\n",
    "    for i in range(iterationNb):\n",
    "        #For each document\n",
    "        for iDoc, doc in enumerate(document):\n",
    "            #For each word in the document\n",
    "            for iToken, wordId in enumerate(doc):\n",
    "                #Initial topic for the token\n",
    "                oldTopic = topicAssignmentList[iDoc][iToken]\n",
    "\n",
    "                #Focus of the i-th Token - decrement in the matrices\n",
    "                documentTopicMatrix[iDoc][oldTopic] -= 1\n",
    "                TopicWordMatrix[oldTopic][wordId] -= 1\n",
    "                \n",
    "                #Gibbs-Sampling\n",
    "                weight = []\n",
    "                for iTopic in range(K):\n",
    "                    #A term\n",
    "                    num_a = topicAssignmentList[iDoc].count(iTopic)+alpha\n",
    "                    denom_a = len(vocab)-1+alpha\n",
    "                    #B term\n",
    "                    num_b = TopicWordMatrix[iTopic][wordId] + eta\n",
    "                    denom_b = TopicWordMatrix.sum(axis=0)[K]+eta\n",
    "                    #Proba\n",
    "                    weight.append((num_a/denom_a)*(num_b/denom_b))\n",
    "                \n",
    "                #Draw topic - multinomial distribution\n",
    "                newTopic = random.choices(range(K), weights = weight, k = 1)[0]\n",
    "                #Re-assign topic\n",
    "                documentTopicMatrix[iDoc][newTopic] += 1\n",
    "                TopicWordMatrix[newTopic][wordId] += 1\n",
    "                topicAssignmentList[iDoc][iToken] = newTopic\n",
    "    #Normalize matrix\n",
    "    documentTopicMatrix = ((documentTopicMatrix+alpha).T/(documentTopicMatrix+alpha).sum(axis=1)).T\n",
    "    TopicWordMatrix = ((TopicWordMatrix+alpha).T/(TopicWordMatrix+alpha).sum(axis=1)).T\n",
    "    return documentTopicMatrix, TopicWordMatrix, topicAssignmentList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentTopicMatrixUpdate, TopicWordMatrixUpdate, topicAssignmentListUpdate = ldaModel(K, alpha, eta, iterationNb, document, vocab, TopicWordMatrix, topicAssignmentList, documentTopicMatrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66129032, 0.33870968],\n",
       "       [0.98611111, 0.01388889],\n",
       "       [0.98387097, 0.01612903],\n",
       "       [0.97619048, 0.02380952],\n",
       "       [0.97619048, 0.02380952],\n",
       "       [0.96875   , 0.03125   ],\n",
       "       [0.88043478, 0.11956522],\n",
       "       [0.34375   , 0.65625   ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentTopicMatrixUpdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02770781, 0.02770781, 0.00251889, 0.00251889, 0.02770781,\n",
       "        0.02770781, 0.02770781, 0.05289673, 0.07808564, 0.02770781,\n",
       "        0.02770781, 0.02770781, 0.10327456, 0.02770781, 0.10327456,\n",
       "        0.07808564, 0.02770781, 0.05289673, 0.00251889, 0.05289673,\n",
       "        0.02770781, 0.02770781, 0.02770781, 0.02770781, 0.02770781,\n",
       "        0.02770781, 0.02770781],\n",
       "       [0.01298701, 0.01298701, 0.14285714, 0.14285714, 0.01298701,\n",
       "        0.01298701, 0.01298701, 0.01298701, 0.01298701, 0.01298701,\n",
       "        0.01298701, 0.01298701, 0.01298701, 0.01298701, 0.01298701,\n",
       "        0.01298701, 0.01298701, 0.01298701, 0.14285714, 0.01298701,\n",
       "        0.01298701, 0.01298701, 0.01298701, 0.01298701, 0.01298701,\n",
       "        0.27272727, 0.01298701]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TopicWordMatrixUpdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 1]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicAssignmentListUpdate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayTopic(TopicWordMatrixUpdate, vocab, nb_word):\n",
    "    vocab = {v: k for k, v in vocab.items()} #Swap id and value to have a dict {id: \"word\"}\n",
    "    for topicNb, wordPerTopic in enumerate(TopicWordMatrixUpdate):\n",
    "        print(f\"\\n>>> Topic {topicNb}\")\n",
    "        TopicWordMatrixSeries = pd.Series(wordPerTopic).sort_values(ascending=False) \n",
    "        wordIds = TopicWordMatrixSeries.index\n",
    "        topicToString = []\n",
    "        for i in range(nb_word):\n",
    "            topicToString.append(f\"{vocab[wordIds[i]]}*{round(TopicWordMatrixSeries[wordIds[i]],2)}\")\n",
    "        print('+'.join(topicToString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Topic 0\n",
      "on*0.1+movie*0.1+holiday*0.08+race*0.08+eat*0.05+thanksgiving*0.05\n",
      "\n",
      ">>> Topic 1\n",
      "turkey*0.27+aspiring*0.14+at*0.14+star*0.14+turtle*0.01+like*0.01\n"
     ]
    }
   ],
   "source": [
    "displayTopic(TopicWordMatrixUpdate, vocab, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
